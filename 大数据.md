# 第1章 大数据技术概述 #
# 第2章 Linux系统的安装和使用 #
调节分辨率

	sudo apt-get install virtualbox-guest-dkms
安装ssh

	sudo apt-get install openssh-server
##创建Hadoop用户##

	sudo useradd -m hadoop -s /bin/bash
设置Hadoop密码

	sudo passwd hadoop
	sxtwan1314
为Hadoop增加管理员权限

	sudo adduser hadoop sudo
解压缩命令

	sudo tar -zxf /home/hadoop/下载/hbase-2.2.2-bin.tar.gz –C /usr/local
本书所有大数据软件都会安装到/usr/local目录下，为了Hadoop有足够权限，因此要给足其权限。权限命令如下：

	sudo chown -R hadoop /usr/local/hbase
更新APT

	sudo apt-get update
vim编辑器安装

	sudo apt-get install vim
	执行过程中摁“y”
 本书下载的内容都下载在/home/hadoop/下载 目录里面 

# 第3章 Hadoop的安装和使用 #
更新APT

	sudo apt-get update
##安装ssh服务端##

	sudo apt-get install openssh-server
登录ssh
	
	ssh localhost
	输入 yes
	输入Hadoop密码
##ssh无密钥登录##

	输入exit退出刚刚的ssh回到终端窗口，然后利用ssh-keygen生成密钥，并将其加入授权中
	cd ~/.ssh/        # 若没有该目录，请先执行一次ssh localhost
	ssh-keygen -t rsa    # 会有提示，都按回车即可
	cat ./id_rsa.pub >> ./authorized_keys  # 加入授权
##安装JDK##
	
	将JDK下载好后放置Downloads里面
	cd /usr/lib
	sudo mkdir jvm #创建/usr/lib/jvm目录用来存放JDK文件

	cd ~ #进入hadoop用户的主目录
	cd Downloads
	
	解压缩JDK
	sudo tar -zxvf ./jdk-8u162-linux-x64.tar.gz -C /usr/lib/jvm
设置环境变量

	vim ~/.bashrc

	配置环境变量
	export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162
	export JRE_HOME=${JAVA_HOME}/jre
	export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
	export PATH=${JAVA_HOME}/bin:$PATH

	让.bashrc立即生效
	source ~/.bashrc

	查看JDK安装是否成功
	java -version
##安装Hadoop##

	下载Hadoop压缩包
	我的版本号是3.3.4
	下载地址https://mirrors.cnnic.cn/apache/hadoop/common/hadoop-3.3.4/

	解压缩 sudo tar -zxf ~/下载/hadoop-3.3.4.tar.gz -C /usr/local    # 解压到/usr/local中
	
	cd /usr/local/
	sudo mv ./hadoop-3.3.4/ ./hadoop（用户）      # 将文件夹名改为hadoop
	sudo chown -R hadoop ./hadoop（文件）       # 修改文件权限
检查Hadoop是否可用

	cd /usr/local/hadoop
	./bin/hadoop version
单机模式配置

	cd /usr/local/hadoop
	./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar

----------

	An example program must be given as the first argument. Valid program names are:
	aggregatewordcount:AnAggregate based map/reduce program that counts the words in the input files,
	aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files. bbp:A map/reduce program that uses Bailey-Borwein Plouffe to compute exact digits of Pi. dbcount:An example job that count the pageview counts from a database.
	distbbp:Amap/reduce program that uses a BBp-type formula to compute exact bits of pi. qrep:Amap/reduce program that counts the matches of a regex in the input. join:A job that effects a join over sorted, equally partitioned datasets multifilewc: A job that counts words from several files.
	pentomino:Amap/reduce tile laving program to find solutions to pentomino problems. pt:Amap/reduce program that estimates pi using a quast-Monte carlo method.
	randomtextwriter:Amap/reduceprogram that writes 10GB of random textual data per node. randomwriter:Amap/reduce program that writes 10GB of random data per node. secondarysort: An example defining a secondary sort to the reduce.
	sort:Amap/reduce program that sorts the data written by the random writer. sudoku: A sudoku solver.
	teragen: Generate data for the terasort terasort: Run the terasort
	teravalidate: checking results of terasort
	wordcount:Amap/reduce program that counts the words in the input files.
	wordmean:A map/reduce program that counts the averaae lenath of the words in the input files, wordmedian:Amap/reduce program that counts the median length of the words in the input files.
	wordstandarddeviation:Aman/reduceproaram that counts the standard deviation of the lenath of the words in the inout files.

	上述命令执行后,会显示所有例子的简介信息,包括grep、join、wordcount 等。这里选择运行grep例子,可以先在/usr/local/hadoop 目录下创建一个文件夹input,并复制一些文件到该文件夹下;然后,运行grep 程序,将input文件夹中的所有文件作为 grep 的输人,让 grep 程序从所有文件中筛选出符合正则表达式“dfs[a-z.]+”的单词,并统计单词出现的次数;最后,把统计结果输出到/usr/local/hadoop/output 文件夹中。
	完成上述操作的具体命令如下:
	
	cd /usr/local/hadoop
	mkdir input
	cp ./etc/hadoop/*.xml ./input   # 将配置文件复制到input目录下
	./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'
	cat ./output/*          # 查看运行结果

	1		dfsadmin  #出现一次表示成功

	需要注意的是，Hadoop默认不会覆盖结果文件，因此再次运行上面实例会提示出错。如果要再次运行，需要先使用如下命令吧output删除
	rm -r ./output
##伪分布式模式配置##

	Hadoop的配置文件位于/usr/local/hadoop/etc/hadoop/
	需要修改配置文件	core-site.xml
					  hdfs-site.xml
	vim打开文件初始内容如下
	<configuration>
	</configuration>
	
	vim core-site.xml
	vim hdfs-site.xml

----------
	修改后的内容如下：
	core-site.xml
	<configuration>
	    <property>
	        <name>hadoop.tmp.dir</name>
	        <value>file:/usr/local/hadoop/tmp</value>
	        <description>Abase for other temporary directories.</description>
	    </property>
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://localhost:9000</value>
	    </property>
	</configuration>
	
	hdfs-site.xml
	<configuration>
	    <property>
	        <name>dfs.replication</name>
	        <value>1</value>
	    </property>
	    <property>
	        <name>dfs.namenode.name.dir</name>
	        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
	    </property>
	    <property>
	        <name>dfs.datanode.data.dir</name>
	        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
	    </property>
	</configuration>

	需要指出的是，Hadoop的运行方式是由配置文件决定的（运行单机模式和伪分布式模式），启动Hadoop时会读取配置文件，然后根据配置文件来决定运行在什么模式下。
	因此，如果需要从伪分布式模式切换回单机模式，只需要删除core-site.xml中的配置即可。

----------
	执行名称节点格式化
	cd /usr/local/hadoop
	./bin/hdfs namenode -format
	格式化成功会看到倒数第十行左右由successfully formatted的提示信息

----------
	启动Hadoop
	cd /usr/local/hadoop
	./sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格
	 Hadoop安装完毕
使用web查看hdfs信息

	http://localhost:9870 
##Hadoop伪分布式实例##

	首先在hdfs中创建用户目录
	cd /usr/local/hadoop
	./bin/hdfs dfs -mkdir -p /user/hadoop

	cd /usr/local/hadoop
	./bin/hdfs dfs -mkdir input  #在HDFS中创建hadoop用户对应的input目录
	./bin/hdfs dfs -put ./etc/hadoop/*.xml input  #把本地文件复制到HDFS中
	
	复制完成后，可以通过如下命令查看hdfs文件列表
	./bin/hdfs dfs -ls input
	
	运行Hadoop自带的grep程序
	./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep input output 'dfs[a-z.]+'

	./bin/hdfs dfs -cat output/*

	./bin/hdfs dfs -rm -r output    # 删除 output 文件夹

	关闭Hadoop
	cd /usr/local/hadoop
	./sbin/stop-dfs.sh

	快速启动Hadoop
	vim ~./bashrc
	在配置文件中加入
	export PATH=$PATH:/usr/local/hadoop/sbin
	export PATH=$PATH:/usr/local/hadoop/sbin:/usr/local/hadoop/bin
	source ~/.bashrc #立即生效






